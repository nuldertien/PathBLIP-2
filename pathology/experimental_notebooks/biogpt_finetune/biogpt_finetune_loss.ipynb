{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/experiment/biogpt_finetune.json\") as f:\n",
    "    biogpt_finetune = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./train_data.json\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open(\"./val_data.json\") as f:\n",
    "    val = json.load(f)\n",
    "\n",
    "with open(\"./test_data.json\") as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(lst_with_reports: list):\n",
    "    reports_he = []\n",
    "    reports_full = []\n",
    "    for specimen in lst_with_reports:\n",
    "        reports_he.append(specimen['caption'])\n",
    "        reports_full.append(specimen['caption_all'])\n",
    "    return reports_he, reports_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_he, train_full = extract_captions(train)\n",
    "val_he, val_full = extract_captions(val)\n",
    "test_he, test_full = extract_captions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexible_write(lst, file_name=\"output.txt\"):\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for item in lst:\n",
    "            file.write(f\"{item}\\n\")\n",
    "\n",
    "flexible_write(train_he, file_name=\"train_he.txt\")\n",
    "flexible_write(train_full, file_name=\"train_full.txt\")\n",
    "\n",
    "flexible_write(val_he, file_name=\"val_he.txt\")\n",
    "flexible_write(val_full, file_name=\"val_full.txt\")\n",
    "\n",
    "flexible_write(test_he, file_name=\"test_he.txt\")\n",
    "flexible_write(test_full, file_name=\"test_full.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "def main():\n",
    "    print(\"=== Starting main() ===\")\n",
    "    data_files = {\n",
    "        \"train\": \"train_he.txt\",\n",
    "        \"validation\": \"val_he.txt\",\n",
    "        \"test\": \"test_he.txt\"\n",
    "    }\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # 1. Load Raw Data (non-streaming so we can get lengths)\n",
    "    # -------------------------------------------------\n",
    "    print(\"Loading raw datasets from files:\", data_files)\n",
    "    raw_datasets = load_dataset(\"text\", data_files=data_files, streaming=False)\n",
    "    print(\"Raw datasets loaded successfully.\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2. Load Model & Tokenizer\n",
    "    # -------------------------------------------------\n",
    "    model_name = \"microsoft/biogpt\"  # or \"microsoft/BioGPT-Large\"\n",
    "    print(\"Loading tokenizer and model:\", model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Pad token not found. Using EOS token as pad token.\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Freeze all parameters except for the output projection\n",
    "    print(\"Freezing model parameters except output projection...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.output_projection.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.output_projection.parameters():\n",
    "        print(f\"The last layer of BioGPT is trainable: {param.requires_grad}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()  # set model to training mode\n",
    "    print(f\"Model moved to device: {device}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3. Tokenize the Data\n",
    "    # -------------------------------------------------\n",
    "    print(\"Starting tokenization of datasets.\")\n",
    "    def tokenize_function(examples):\n",
    "        # Add an explicit EOS token to the text\n",
    "        text_with_eos = [text + \" \" + tokenizer.eos_token for text in examples[\"text\"]]\n",
    "        return tokenizer(text_with_eos)\n",
    "\n",
    "    # We remove the original \"text\" column\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    print(\"Tokenization complete.\")\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    val_dataset   = tokenized_datasets[\"validation\"]\n",
    "    test_dataset  = tokenized_datasets[\"test\"]\n",
    "    print(\"Datasets split into train, validation, and test.\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4. Prepare Dataloaders\n",
    "    # -------------------------------------------------\n",
    "    print(\"Preparing dataloaders...\")\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True,  collate_fn=data_collator)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=75, shuffle=False, collate_fn=data_collator)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=75, shuffle=False, collate_fn=data_collator)\n",
    "    print(\"Dataloaders are ready.\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5. Define Optimizer \n",
    "    # -------------------------------------------------\n",
    "    print(\"Setting up optimizer.\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, betas=(0.9, 0.99))\n",
    "    print(\"Optimizer is ready.\")\n",
    "\n",
    "    # Ensure the parent folder \"3\" exists\n",
    "    parent_folder = \"best_model3\"\n",
    "    os.makedirs(parent_folder, exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 6. Training Loop (saving model per epoch)\n",
    "    # -------------------------------------------------\n",
    "    num_epochs = 5\n",
    "\n",
    "    print(\"Starting training loop...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- Epoch {epoch+1} starting ---\")\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"input_ids\"]\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, step {step+1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(total_train_loss)\n",
    "        print(f\"Epoch {epoch+1}: average training loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Validation\n",
    "        # -------------------------------------------------\n",
    "        print(f\"Starting validation for epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"input_ids\"]\n",
    "                )\n",
    "                total_val_loss += outputs.loss.item()\n",
    "\n",
    "        val_loss = total_val_loss / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}: validation loss = {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model checkpoint for this epoch inside folder \"3\"\n",
    "        current_model_path = os.path.join(parent_folder, f\"best_model3.{epoch+1}\")\n",
    "        print(f\"Saving model checkpoint for epoch {epoch+1} to {current_model_path}\")\n",
    "        model.save_pretrained(current_model_path)\n",
    "        tokenizer.save_pretrained(current_model_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main(output_file, best_model_path):\n",
    "    print(\"Loading the best model for final test evaluation...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model in FP16 mode for faster inference on A100\n",
    "    best_model = AutoModelForCausalLM.from_pretrained(\n",
    "        best_model_path, \n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\"  # Automatically places layers on the GPU\n",
    "    )\n",
    "    best_model.to(device)\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"Generating predictions for each test instance...\")\n",
    "    num_test_samples = 1970\n",
    "    print(f\"Number of test instances: {num_test_samples}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "    \n",
    "    # Encode the prompt once (here an empty string)\n",
    "    prompt_text = \"\"\n",
    "    encoded_prompt = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Set batch size based on available GPU memory\n",
    "    batch_size = 16  # Adjust this value as needed for your A100\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(0, num_test_samples, batch_size), desc=\"Generating reports\", unit=\"batch\"):\n",
    "            current_batch_size = min(batch_size, num_test_samples - i)\n",
    "            # Replicate the prompt for the current batch\n",
    "            input_ids = encoded_prompt.repeat(current_batch_size, 1)\n",
    "            with torch.no_grad():\n",
    "                outputs = best_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    num_beams=4,  # Beam search for better outputs\n",
    "                    max_new_tokens=1024,\n",
    "                    top_p=1.0,\n",
    "                    repetition_penalty=1.2,\n",
    "                    length_penalty=1.1,\n",
    "                    do_sample=False,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            # Decode and write outputs for each sample in the batch\n",
    "            for output in outputs:\n",
    "                predicted_caption = tokenizer.decode(output, skip_special_tokens=True)\n",
    "                f.write(predicted_caption + \"\\n\")\n",
    "\n",
    "    print(f\"Predictions for all {num_test_samples} test instances have been saved to '{output_file}'.\")\n",
    "    print(\"=== Finished main() ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_file = \"generated_captions.txt\"\n",
    "    best_model_path = \"best_model\"\n",
    "    main(output_file, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu_scorer import BleuScorer\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def compute_scores(gts, res): \n",
    "    \"\"\"\n",
    "    Performs the MS COCO evaluation using the Python 3 implementation (https://github.com/salaniz/pycocoevalcap)\n",
    "\n",
    "    :param gts: Dictionary with the image ids and their gold captions,\n",
    "    :param res: Dictionary with the image ids and their generated captions\n",
    "    :print: Evaluation score (the mean of the scores of all the instances) for each measure\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up scorers\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"BLEU_1\", \"BLEU_2\", \"BLEU_3\", \"BLEU_4\"]),\n",
    "        (Meteor(), \"METEOR\"), # sudo apt-get install default-jre <- to install java and not return error\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "        # (Spice(), \"SPICE\") \n",
    "    ]\n",
    "\n",
    "    eval_res = {}\n",
    "    eval_res_total = {}\n",
    "    # Compute score for each metric\n",
    "    for scorer, method in scorers:\n",
    "        try:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        except TypeError:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        if type(method) == list:\n",
    "            for sc, m in zip(score, method):\n",
    "                eval_res[m] = sc\n",
    "            for sc, m in zip(scores, method):\n",
    "                eval_res_total[m] = sc\n",
    "        else:\n",
    "            eval_res[method] = score\n",
    "            eval_res_total[method] = scores\n",
    "\n",
    "\n",
    "    return eval_res, eval_res_total\n",
    "\n",
    "# The following code is taken from the pycocoevalcap library\n",
    "# It is needed to put bleu_scorer.compute_score to verbose = 0\n",
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        # default compute Blue score up to 4\n",
    "        self._n = n\n",
    "        self._hypo_for_image = {}\n",
    "        self.ref_for_image = {}\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        bleu_scorer = BleuScorer(n=self._n)\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) >= 1)\n",
    "\n",
    "            bleu_scorer += (hypo[0], ref)\n",
    "\n",
    "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
    "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
    "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
    "        \n",
    "        # return (bleu, bleu_info)\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"Bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_he.txt', 'r') as file:\n",
    "    ground_truth = [line.strip() for line in file]\n",
    "\n",
    "with open('generated_captions_he_epoch_5_5e-5_sample_true.txt', 'r') as file:\n",
    "    generated_captions = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "\n",
    "for gts, res in zip(ground_truth, generated_captions):\n",
    "    eval_results.append({'gt_caption':gts, 'caption':res})\n",
    "\n",
    "gts = {i: [gt] for i, gt in enumerate([x['gt_caption'] for x in eval_results])}\n",
    "res = {i: [re] for i, re in enumerate([x['caption'] for x in eval_results])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res, eval_res_total = compute_scores(gts, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
