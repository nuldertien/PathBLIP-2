{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.bleu.bleu_scorer import BleuScorer\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def compute_scores(gts, res): \n",
    "    \"\"\"\n",
    "    Performs the MS COCO evaluation using the Python 3 implementation (https://github.com/salaniz/pycocoevalcap)\n",
    "\n",
    "    :param gts: Dictionary with the image ids and their gold captions,\n",
    "    :param res: Dictionary with the image ids and their generated captions\n",
    "    :print: Evaluation score (the mean of the scores of all the instances) for each measure\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up scorers\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"BLEU_1\", \"BLEU_2\", \"BLEU_3\", \"BLEU_4\"]),\n",
    "        (Meteor(), \"METEOR\"), # sudo apt-get install default-jre <- to install java and not return error\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "        # (Spice(), \"SPICE\") \n",
    "    ]\n",
    "\n",
    "    eval_res = {}\n",
    "    eval_res_total = {}\n",
    "    # Compute score for each metric\n",
    "    for scorer, method in scorers:\n",
    "        try:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        except TypeError:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        if type(method) == list:\n",
    "            for sc, m in zip(score, method):\n",
    "                eval_res[m] = sc\n",
    "            for sc, m in zip(scores, method):\n",
    "                eval_res_total[m] = sc\n",
    "        else:\n",
    "            eval_res[method] = score\n",
    "            eval_res_total[method] = scores\n",
    "\n",
    "\n",
    "    return eval_res, eval_res_total\n",
    "\n",
    "# The following code is taken from the pycocoevalcap library\n",
    "# It is needed to put bleu_scorer.compute_score to verbose = 0\n",
    "\n",
    "def compute_scores_bleu(gts, res): \n",
    "    \"\"\"\n",
    "    Performs the MS COCO evaluation using the Python 3 implementation (https://github.com/salaniz/pycocoevalcap)\n",
    "\n",
    "    :param gts: Dictionary with the image ids and their gold captions,\n",
    "    :param res: Dictionary with the image ids and their generated captions\n",
    "    :print: Evaluation score (the mean of the scores of all the instances) for each measure\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up scorers\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"BLEU_1\", \"BLEU_2\", \"BLEU_3\", \"BLEU_4\"]),\n",
    "        (Meteor(), \"METEOR\")\n",
    "    ]\n",
    "\n",
    "    eval_res = {}\n",
    "    eval_res_total = {}\n",
    "    # Compute score for each metric\n",
    "    for scorer, method in scorers:\n",
    "        try:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        except TypeError:\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "        if type(method) == list:\n",
    "            for sc, m in zip(score, method):\n",
    "                eval_res[m] = sc\n",
    "            for sc, m in zip(scores, method):\n",
    "                eval_res_total[m] = sc\n",
    "        else:\n",
    "            eval_res[method] = score\n",
    "            eval_res_total[method] = scores\n",
    "\n",
    "    return eval_res, eval_res_total\n",
    "\n",
    "# calculate bleu's total, common and non_common corpus average\n",
    "\n",
    "def calculate_bleu_t_c_nc(df, second_column):\n",
    "    gts = {i: [val] for i, val in enumerate(df[\"gt_caption\"])}\n",
    "    res = {i: [val] for i, val in enumerate(df[second_column])}\n",
    "    total_bleu_average, _ = compute_scores_bleu(gts, res)\n",
    "\n",
    "    # Common bleu corpus average\n",
    "    common_df = df[df['label']==False]\n",
    "    gts_common = {i: [val] for i, val in enumerate(common_df[\"gt_caption\"])}\n",
    "    res_common = {i: [val] for i, val in enumerate(common_df[second_column])}\n",
    "    common_bleu_average, _ = compute_scores_bleu(gts_common, res_common)\n",
    "\n",
    "    # Non-common bleu corpus average\n",
    "    non_common_df = df[df['label']==True]\n",
    "    gts_non_common = {i: [val] for i, val in enumerate(non_common_df[\"gt_caption\"])}\n",
    "    res_non_common = {i: [val] for i, val in enumerate(non_common_df[second_column])}\n",
    "    non_common_bleu_average, _ = compute_scores_bleu(gts_non_common, res_non_common)\n",
    "\n",
    "    complete = {\n",
    "        'total': {\n",
    "            'BLEU_1': total_bleu_average['BLEU_1'], \n",
    "            'BLEU_4': total_bleu_average['BLEU_4'],\n",
    "            'METEOR': total_bleu_average['METEOR']\n",
    "            },\n",
    "        'common': {\n",
    "            'BLEU_1': common_bleu_average['BLEU_1'], \n",
    "            'BLEU_4': common_bleu_average['BLEU_4'],\n",
    "            'METEOR': common_bleu_average['METEOR']\n",
    "            },\n",
    "        'non_common': {\n",
    "            'BLEU_1': non_common_bleu_average['BLEU_1'], \n",
    "            'BLEU_4': non_common_bleu_average['BLEU_4'],\n",
    "            'METEOR': non_common_bleu_average['METEOR']\n",
    "            }\n",
    "    }\n",
    "    return complete\n",
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        # default compute Blue score up to 4\n",
    "        self._n = n\n",
    "        self._hypo_for_image = {}\n",
    "        self.ref_for_image = {}\n",
    "\n",
    "    def compute_score(self, gts, res):\n",
    "\n",
    "        assert(gts.keys() == res.keys())\n",
    "        imgIds = gts.keys()\n",
    "\n",
    "        bleu_scorer = BleuScorer(n=self._n)\n",
    "        for id in imgIds:\n",
    "            hypo = res[id]\n",
    "            ref = gts[id]\n",
    "\n",
    "            # Sanity check.\n",
    "            assert(type(hypo) is list)\n",
    "            assert(len(hypo) == 1)\n",
    "            assert(type(ref) is list)\n",
    "            assert(len(ref) >= 1)\n",
    "\n",
    "            bleu_scorer += (hypo[0], ref)\n",
    "\n",
    "        #score, scores = bleu_scorer.compute_score(option='shortest')\n",
    "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)\n",
    "        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n",
    "        \n",
    "        # return (bleu, bleu_info)\n",
    "        return score, scores\n",
    "\n",
    "    def method(self):\n",
    "        return \"Bleu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/experiment/biogpt.json\") as f:\n",
    "    biogpt_he = json.load(f)\n",
    "\n",
    "with open(\"./data/experiment/biogpt-all.json\") as f:\n",
    "    biogpt_all = json.load(f)\n",
    "\n",
    "with open(\"./data/experiment/generated_captions_he.txt\") as f:\n",
    "    baseline_he = f.readlines()\n",
    "\n",
    "with open(\"./data/experiment/generated_captions_full.txt\") as f:\n",
    "    baseline_all = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "biogpt_he_df = pd.DataFrame(biogpt_he).set_index('image_id').rename(columns={'caption':'biogpt_he_caption'})\n",
    "biogpt_all_df = pd.DataFrame(biogpt_all).set_index('image_id').rename(columns={'caption':'biogpt_all_caption'})\n",
    "\n",
    "biogpt_he_df['baseline_he'] = baseline_he\n",
    "biogpt_all_df['baseline_all'] = baseline_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def calculate_the_captioning_scores_in_batches(\n",
    "    df: pd.DataFrame, \n",
    "    second_column: str,\n",
    "    batch_size: int = 32\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes captioning scores in batches and assigns the results to\n",
    "    a new column in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the columns 'gt_caption' and `second_column`.\n",
    "    second_column : str\n",
    "        The name of the column that holds the generated captions for scoring.\n",
    "    compute_scores_function : callable\n",
    "        A function that takes in (gts, res) and returns (eval_res, eval_res_total).\n",
    "        It's assumed to have the same signature as your original `compute_scores`.\n",
    "    batch_size : int, optional\n",
    "        The size of each batch to process, by default 32.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The same DataFrame with an additional column \n",
    "        f\"{second_column}_eval_results\" containing per-image scoring results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare a list to hold the per-row dictionaries for the entire DataFrame\n",
    "    all_instance_results = [None] * len(df)\n",
    "\n",
    "    # Iterate over the DataFrame in chunks of size `batch_size`\n",
    "    for start_idx in tqdm.tqdm(range(0, len(df), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        df_subset = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Build the per-batch dictionaries of ground truth and generated captions\n",
    "        # Notice the 0..N indexing within this subset for `compute_scores`\n",
    "        local_gts = {i: [val] for i, val in enumerate(df_subset[\"gt_caption\"])}\n",
    "        local_res = {i: [val] for i, val in enumerate(df_subset[second_column])}\n",
    "\n",
    "        # Compute the scores on this subset\n",
    "        eval_res, eval_res_total = compute_scores(local_gts, local_res)\n",
    "\n",
    "        # Assign the per-image scores into the correct slice of our final results\n",
    "        for local_idx, global_idx in enumerate(range(start_idx, end_idx)):\n",
    "            row_scores = {}\n",
    "            for metric, scores_list in eval_res_total.items():\n",
    "                row_scores[metric] = scores_list[local_idx]\n",
    "            all_instance_results[global_idx] = row_scores\n",
    "\n",
    "    # Assign the final per-image results to the new column\n",
    "    df[f\"{second_column}_eval_results\"] = all_instance_results\n",
    "\n",
    "    # Return the updated DataFrame. If you need any aggregated metric across\n",
    "    # the entire dataset, you can compute or return it separately.\n",
    "    return eval_res, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of biogpt_all_caption: 82.78730964467005\n",
      "Average length of baseline_he: 250.73604060913706\n",
      "Average length of biogpt_he_caption: 60.54619289340101\n",
      "Average length of baseline_he: 158.56751269035533\n"
     ]
    }
   ],
   "source": [
    "biogpt_all_df[\"len_biogpt_all_caption\"] = biogpt_all_df[\"biogpt_all_caption\"].apply(lambda x: len(str(x).split()))\n",
    "biogpt_all_df[\"len_baseline_all\"] = biogpt_all_df[\"baseline_all\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"Average length of biogpt_all_caption:\", biogpt_all_df[\"len_biogpt_all_caption\"].mean())\n",
    "print(\"Average length of baseline_he:\", biogpt_all_df[\"len_baseline_all\"].mean())\n",
    "\n",
    "biogpt_he_df[\"len_biogpt_he_caption\"] = biogpt_he_df[\"biogpt_he_caption\"].apply(lambda x: len(str(x).split()))\n",
    "biogpt_he_df[\"len_baseline_he\"] = biogpt_he_df[\"baseline_he\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"Average length of biogpt_he_caption:\", biogpt_he_df[\"len_biogpt_he_caption\"].mean())\n",
    "print(\"Average length of baseline_he:\", biogpt_he_df[\"len_baseline_he\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "biogpt_he_df['baseline_he'] = biogpt_he_df['baseline_he'].str.replace('\\n', '')\n",
    "biogpt_all_df['baseline_all'] = biogpt_all_df['baseline_all'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.37s/it]\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.89s/it]\n"
     ]
    }
   ],
   "source": [
    "captioning_metrics_biogpt_he, df_he = calculate_the_captioning_scores_in_batches(biogpt_he_df, 'biogpt_he_caption', 1970)\n",
    "captioning_metrics_baseline_he, df_he = calculate_the_captioning_scores_in_batches(df_he, 'baseline_he', 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.21s/it]\n",
      "100%|██████████| 1/1 [01:16<00:00, 76.21s/it]\n"
     ]
    }
   ],
   "source": [
    "captioning_metrics_biogpt_all, df_all = calculate_the_captioning_scores_in_batches(biogpt_all_df, 'biogpt_all_caption', 1970)\n",
    "captioning_metrics_baseline_all, df_all = calculate_the_captioning_scores_in_batches(df_all, 'baseline_all', 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU_1': 0.33895694732434295,\n",
       " 'BLEU_2': 0.22218331259067925,\n",
       " 'BLEU_3': 0.15995441847538674,\n",
       " 'BLEU_4': 0.12387873113049473,\n",
       " 'METEOR': 0.18264103292761122,\n",
       " 'ROUGE_L': 0.28806295350039757,\n",
       " 'CIDEr': 0.35653341827914276}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captioning_metrics_biogpt_he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU_1': 0.3270022918591056,\n",
       " 'BLEU_2': 0.222950965735909,\n",
       " 'BLEU_3': 0.16408986234852505,\n",
       " 'BLEU_4': 0.12737205327226825,\n",
       " 'METEOR': 0.17833666656398903,\n",
       " 'ROUGE_L': 0.3256555116847032,\n",
       " 'CIDEr': 0.2913729757238771}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captioning_metrics_biogpt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/patient_info/patient_characteristics.xlsx', 'rb') as f:\n",
    "    patient_info = pd.read_excel(f)\n",
    "\n",
    "with open('./data/patient_info/report_id_specimen_map.json', 'rb') as f:\n",
    "    report_id_specimen_map = json.load(f)\n",
    "\n",
    "patient_info['specimen'] = patient_info['specimen'].apply(\n",
    "    lambda x: x[:9] + \"_\" + x[9:]\n",
    ")\n",
    "\n",
    "specimen_report_id_map = {v: k for k, v in report_id_specimen_map.items()}\n",
    "mapping = patient_info[['specimen', 'label']].set_index('specimen').to_dict()['label']\n",
    "\n",
    "df_he['specimen'] = df_he.index.map(specimen_report_id_map)\n",
    "df_he['label'] = df_he['specimen'].map(mapping)\n",
    "\n",
    "df_all['specimen'] = df_all.index.map(specimen_report_id_map)\n",
    "df_all['label'] = df_all['specimen'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_metrics_he = pd.DataFrame(df_he['biogpt_he_caption_eval_results'].to_dict()).T\n",
    "captioning_metrics_baseline_he = pd.DataFrame(df_he['baseline_he_eval_results'].to_dict()).T\n",
    "\n",
    "captioning_metrics_all = pd.DataFrame(df_all['biogpt_all_caption_eval_results'].to_dict()).T\n",
    "captioning_metrics_baseline_all = pd.DataFrame(df_all['baseline_all_eval_results'].to_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [\n",
    "    None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_metrics_he['specimen'] = captioning_metrics_he.index.map(specimen_report_id_map)\n",
    "captioning_metrics_he['label'] = captioning_metrics_he['specimen'].map(mapping)\n",
    "\n",
    "captioning_metrics_baseline_he['specimen'] = captioning_metrics_baseline_he.index.map(specimen_report_id_map)\n",
    "captioning_metrics_baseline_he['label'] = captioning_metrics_baseline_he['specimen'].map(mapping)\n",
    "\n",
    "captioning_metrics_all['specimen'] = captioning_metrics_all.index.map(specimen_report_id_map)\n",
    "captioning_metrics_all['label'] = captioning_metrics_all['specimen'].map(mapping)\n",
    "\n",
    "captioning_metrics_baseline_all['specimen'] = captioning_metrics_baseline_all.index.map(specimen_report_id_map)\n",
    "captioning_metrics_baseline_all['label'] = captioning_metrics_baseline_all['specimen'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_values = captioning_metrics_he[captioning_metrics_he['specimen'].isin(idx)]\n",
    "experiment_values[experiment_values['label']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    1597\n",
       "True      373\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captioning_metrics_he['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_confidence_intervals(df, second_column, metrics, bootstraps, alpha=0.95):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lower_p = (1 - alpha) / 2 * 100\n",
    "    upper_p = (1 + alpha) / 2 * 100\n",
    "\n",
    "    common_subset = metrics[metrics['label']==False]\n",
    "    non_common_subset = metrics[metrics['label']==True]\n",
    "\n",
    "    results_total = {}\n",
    "    results_common = {}\n",
    "    results_non_common = {}\n",
    "\n",
    "    # Different solution for BLEU, corpus average instead of normal average\n",
    "    bootstrap_values_bleu_1 = {'total': [], 'common': [], 'non_common': []}\n",
    "    bootstrap_values_bleu_4 = {'total': [], 'common': [], 'non_common': []}\n",
    "    bootstrap_values_meteor = {'total': [], 'common': [], 'non_common': []}\n",
    "    for _ in tqdm.tqdm(range(bootstraps)):\n",
    "        # Total bleu corpus average\n",
    "        idx = np.random.choice(df.index, len(df), replace=True)\n",
    "        filtered_df = df.loc[idx]\n",
    "        gts = {i: [val] for i, val in enumerate(filtered_df[\"gt_caption\"])}\n",
    "        res = {i: [val] for i, val in enumerate(filtered_df[second_column])}\n",
    "        total_bleu_average, _ = compute_scores_bleu(gts, res)\n",
    "\n",
    "        # Common bleu corpus average\n",
    "        common_df = df[df['label']==False]\n",
    "        idx_common = np.random.choice(common_df.index, len(common_df), replace=True)\n",
    "        filtered_common_df = df.loc[idx_common]\n",
    "        gts_common = {i: [val] for i, val in enumerate(filtered_common_df[\"gt_caption\"])}\n",
    "        res_common = {i: [val] for i, val in enumerate(filtered_common_df[second_column])}\n",
    "        common_bleu_average, _ = compute_scores_bleu(gts_common, res_common)\n",
    "\n",
    "        # Non-common bleu corpus average\n",
    "        non_common_df = df[df['label']==True]\n",
    "        idx_non_common = np.random.choice(non_common_df.index, len(non_common_df), replace=True)\n",
    "        filtered_non_common_df = df.loc[idx_non_common]\n",
    "        gts_non_common = {i: [val] for i, val in enumerate(filtered_non_common_df[\"gt_caption\"])}\n",
    "        res_non_common = {i: [val] for i, val in enumerate(filtered_non_common_df[second_column])}\n",
    "        non_common_bleu_average, _ = compute_scores_bleu(gts_non_common, res_non_common)\n",
    "        \n",
    "        bootstrap_values_bleu_1['total'].append(total_bleu_average['BLEU_1'])\n",
    "        bootstrap_values_bleu_1['common'].append(common_bleu_average['BLEU_1'])\n",
    "        bootstrap_values_bleu_1['non_common'].append(non_common_bleu_average['BLEU_1'])\n",
    "\n",
    "        bootstrap_values_bleu_4['total'].append(total_bleu_average['BLEU_4'])\n",
    "        bootstrap_values_bleu_4['common'].append(common_bleu_average['BLEU_4'])\n",
    "        bootstrap_values_bleu_4['non_common'].append(non_common_bleu_average['BLEU_4'])\n",
    "\n",
    "        bootstrap_values_meteor['total'].append(total_bleu_average['METEOR'])\n",
    "        bootstrap_values_meteor['common'].append(common_bleu_average['METEOR'])\n",
    "        bootstrap_values_meteor['non_common'].append(non_common_bleu_average['METEOR'])\n",
    "        \n",
    "    results_total['BLEU_1'] = (\n",
    "        np.percentile(bootstrap_values_bleu_1['total'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_1['total'], upper_p)\n",
    "    )\n",
    "    results_common['BLEU_1'] = (\n",
    "        np.percentile(bootstrap_values_bleu_1['common'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_1['common'], upper_p)\n",
    "    )\n",
    "    results_non_common['BLEU_1'] = (\n",
    "        np.percentile(bootstrap_values_bleu_1['non_common'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_1['non_common'], upper_p)\n",
    "    )\n",
    "\n",
    "    results_total['BLEU_4'] = (\n",
    "        np.percentile(bootstrap_values_bleu_4['total'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_4['total'], upper_p)\n",
    "    )\n",
    "    results_common['BLEU_4'] = (\n",
    "        np.percentile(bootstrap_values_bleu_4['common'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_4['common'], upper_p)\n",
    "    )\n",
    "    results_non_common['BLEU_4'] = (\n",
    "        np.percentile(bootstrap_values_bleu_4['non_common'], lower_p),\n",
    "        np.percentile(bootstrap_values_bleu_4['non_common'], upper_p)\n",
    "    )\n",
    "\n",
    "    results_total['METEOR'] = (\n",
    "        np.percentile(bootstrap_values_meteor['total'], lower_p),\n",
    "        np.percentile(bootstrap_values_meteor['total'], upper_p)\n",
    "    )\n",
    "    results_common['METEOR'] = (\n",
    "        np.percentile(bootstrap_values_meteor['common'], lower_p),\n",
    "        np.percentile(bootstrap_values_meteor['common'], upper_p)\n",
    "    )\n",
    "    results_non_common['METEOR'] = (\n",
    "        np.percentile(bootstrap_values_meteor['non_common'], lower_p),\n",
    "        np.percentile(bootstrap_values_meteor['non_common'], upper_p)\n",
    "    )\n",
    "\n",
    "    metric_list = [\"ROUGE_L\",\"CIDEr\"]\n",
    "    for metric in metric_list:\n",
    "        metric_values = metrics[metric].values\n",
    "        common_values = common_subset[metric].values\n",
    "        non_common_values = non_common_subset[metric].values\n",
    "        bootstrap_values = {'total': [], 'common': [], 'non_common': []}\n",
    "        for _ in range(bootstraps):\n",
    "            bootstrap_values['total'].append(np.mean(\n",
    "                np.random.choice(metric_values, \n",
    "                                 len(metric_values), \n",
    "                                 replace=True))\n",
    "                                 )\n",
    "            bootstrap_values['common'].append(np.mean(\n",
    "                np.random.choice(common_values, \n",
    "                                 len(common_subset), \n",
    "                                 replace=True))\n",
    "                                 )\n",
    "            bootstrap_values['non_common'].append(np.mean(\n",
    "                np.random.choice(non_common_values, \n",
    "                                 len(non_common_subset), \n",
    "                                 replace=True))\n",
    "                                 )\n",
    "        results_total[metric] = (\n",
    "            np.percentile(bootstrap_values['total'], lower_p),\n",
    "            np.percentile(bootstrap_values['total'], upper_p)\n",
    "        )\n",
    "        results_common[metric] = (\n",
    "            np.percentile(bootstrap_values['common'], lower_p),\n",
    "            np.percentile(bootstrap_values['common'], upper_p)\n",
    "        )\n",
    "        results_non_common[metric] = (\n",
    "            np.percentile(bootstrap_values['non_common'], lower_p),\n",
    "            np.percentile(bootstrap_values['non_common'], upper_p)\n",
    "        )\n",
    "\n",
    "\n",
    "    results_total = pd.DataFrame(results_total, index=['lower', 'upper'])\n",
    "    results_common = pd.DataFrame(results_common, index=['lower', 'upper'])\n",
    "    results_non_common = pd.DataFrame(results_non_common, index=['lower', 'upper'])\n",
    "\n",
    "    return results_total, results_common, results_non_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [9:45:57<00:00, 35.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "total_he, common_he, non_common_he = bootstrap_confidence_intervals(df_he, 'biogpt_he_caption', captioning_metrics_he, n_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_scores = calculate_bleu_t_c_nc(df_he, 'biogpt_he_caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_metrics = ['ROUGE_L', 'CIDEr']\n",
    "\n",
    "he_result = {**captioning_metrics_he[cap_metrics].mean().to_dict(), **he_scores['total']}\n",
    "common_he_result = {**captioning_metrics_he.groupby('label')[cap_metrics].mean().T.to_dict()[False], **he_scores['common']}\n",
    "non_common_he_result = {**captioning_metrics_he.groupby('label')[cap_metrics].mean().T.to_dict()[True], **he_scores['non_common']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:47<00:00, 47.19s/it]\n"
     ]
    }
   ],
   "source": [
    "total_he_baseline, common_he_baseline, non_common_he_baseline = bootstrap_confidence_intervals(df_he, 'baseline_he', captioning_metrics_baseline_he, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_baseline_scores = calculate_bleu_t_c_nc(df_he, 'baseline_he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_baseline_result = {**captioning_metrics_baseline_he[cap_metrics].mean().to_dict(), **he_baseline_scores['total']}\n",
    "common_he_baseline_result = {**captioning_metrics_baseline_he.groupby('label')[cap_metrics].mean().T.to_dict()[False], **he_baseline_scores['common']}\n",
    "non_common_he_baseline_result = {**captioning_metrics_baseline_he.groupby('label')[cap_metrics].mean().T.to_dict()[True], **he_baseline_scores['non_common']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [13:05:18<00:00, 47.12s/it] \n"
     ]
    }
   ],
   "source": [
    "total_all, common_all, non_common_all = bootstrap_confidence_intervals(df_all, \"biogpt_all_caption\", captioning_metrics_all, n_bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = calculate_bleu_t_c_nc(df_all, 'biogpt_all_caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result = {**captioning_metrics_all[cap_metrics].mean().to_dict(), **all_scores['total']}\n",
    "common_all_result = {**captioning_metrics_all.groupby('label')[cap_metrics].mean().T.to_dict()[False], **all_scores['common']}\n",
    "non_common_all_result = {**captioning_metrics_all.groupby('label')[cap_metrics].mean().T.to_dict()[True], **all_scores['non_common']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:13<00:00, 73.40s/it]\n"
     ]
    }
   ],
   "source": [
    "total_all_baseline, common_all_baseline, non_common_all_baseline = bootstrap_confidence_intervals(df_all, 'baseline_all', captioning_metrics_baseline_all, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_baseline_scores = calculate_bleu_t_c_nc(df_all, 'baseline_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_baseline_result = {**captioning_metrics_baseline_all[cap_metrics].mean().to_dict(), **all_baseline_scores['total']}\n",
    "common_all_baseline_result = {**captioning_metrics_baseline_all.groupby('label')[cap_metrics].mean().T.to_dict()[False], **all_baseline_scores['common']}\n",
    "non_common_all_baseline_result = {**captioning_metrics_baseline_all.groupby('label')[cap_metrics].mean().T.to_dict()[True], **all_baseline_scores['non_common']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>BLEU_1</th>\n",
       "      <th>BLEU_4</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>ROUGE_L</th>\n",
       "      <th>CIDEr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Subset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">biogpt_he</th>\n",
       "      <th>Total</th>\n",
       "      <td>0.339 (0.331 - 0.347)</td>\n",
       "      <td>0.124 (0.117 - 0.131)</td>\n",
       "      <td>0.183 (0.179 - 0.186)</td>\n",
       "      <td>0.288 (0.280 - 0.296)</td>\n",
       "      <td>0.357 (0.301 - 0.413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Common</th>\n",
       "      <td>0.357 (0.349 - 0.367)</td>\n",
       "      <td>0.138 (0.129 - 0.147)</td>\n",
       "      <td>0.194 (0.190 - 0.198)</td>\n",
       "      <td>0.303 (0.294 - 0.312)</td>\n",
       "      <td>0.424 (0.354 - 0.493)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-common</th>\n",
       "      <td>0.289 (0.275 - 0.304)</td>\n",
       "      <td>0.086 (0.076 - 0.097)</td>\n",
       "      <td>0.154 (0.149 - 0.160)</td>\n",
       "      <td>0.224 (0.214 - 0.235)</td>\n",
       "      <td>0.069 (0.042 - 0.109)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">baseline_he</th>\n",
       "      <th>Total</th>\n",
       "      <td>0.203 (0.203 - 0.203)</td>\n",
       "      <td>0.031 (0.031 - 0.031)</td>\n",
       "      <td>0.186 (0.185 - 0.185)</td>\n",
       "      <td>0.163 (0.161 - 0.161)</td>\n",
       "      <td>0.012 (0.012 - 0.012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Common</th>\n",
       "      <td>0.192 (0.191 - 0.191)</td>\n",
       "      <td>0.031 (0.030 - 0.030)</td>\n",
       "      <td>0.191 (0.189 - 0.189)</td>\n",
       "      <td>0.166 (0.168 - 0.168)</td>\n",
       "      <td>0.013 (0.013 - 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-common</th>\n",
       "      <td>0.250 (0.252 - 0.252)</td>\n",
       "      <td>0.030 (0.031 - 0.031)</td>\n",
       "      <td>0.170 (0.171 - 0.171)</td>\n",
       "      <td>0.152 (0.153 - 0.153)</td>\n",
       "      <td>0.008 (0.008 - 0.008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">biogpt_all</th>\n",
       "      <th>Total</th>\n",
       "      <td>0.327 (0.317 - 0.337)</td>\n",
       "      <td>0.127 (0.121 - 0.134)</td>\n",
       "      <td>0.178 (0.174 - 0.183)</td>\n",
       "      <td>0.326 (0.319 - 0.333)</td>\n",
       "      <td>0.291 (0.249 - 0.339)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Common</th>\n",
       "      <td>0.371 (0.362 - 0.381)</td>\n",
       "      <td>0.150 (0.142 - 0.157)</td>\n",
       "      <td>0.199 (0.194 - 0.203)</td>\n",
       "      <td>0.349 (0.341 - 0.356)</td>\n",
       "      <td>0.343 (0.289 - 0.404)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-common</th>\n",
       "      <td>0.242 (0.219 - 0.263)</td>\n",
       "      <td>0.085 (0.073 - 0.095)</td>\n",
       "      <td>0.140 (0.132 - 0.147)</td>\n",
       "      <td>0.226 (0.214 - 0.240)</td>\n",
       "      <td>0.068 (0.045 - 0.095)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">baseline_all</th>\n",
       "      <th>Total</th>\n",
       "      <td>0.180 (0.175 - 0.175)</td>\n",
       "      <td>0.029 (0.028 - 0.028)</td>\n",
       "      <td>0.167 (0.164 - 0.164)</td>\n",
       "      <td>0.159 (0.160 - 0.160)</td>\n",
       "      <td>0.010 (0.009 - 0.009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Common</th>\n",
       "      <td>0.166 (0.167 - 0.167)</td>\n",
       "      <td>0.028 (0.028 - 0.028)</td>\n",
       "      <td>0.177 (0.178 - 0.178)</td>\n",
       "      <td>0.163 (0.162 - 0.162)</td>\n",
       "      <td>0.010 (0.010 - 0.010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-common</th>\n",
       "      <td>0.239 (0.236 - 0.236)</td>\n",
       "      <td>0.032 (0.031 - 0.031)</td>\n",
       "      <td>0.144 (0.146 - 0.146)</td>\n",
       "      <td>0.140 (0.139 - 0.139)</td>\n",
       "      <td>0.009 (0.009 - 0.009)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        BLEU_1                 BLEU_4  \\\n",
       "Model        Subset                                                     \n",
       "biogpt_he    Total       0.339 (0.331 - 0.347)  0.124 (0.117 - 0.131)   \n",
       "             Common      0.357 (0.349 - 0.367)  0.138 (0.129 - 0.147)   \n",
       "             Non-common  0.289 (0.275 - 0.304)  0.086 (0.076 - 0.097)   \n",
       "baseline_he  Total       0.203 (0.203 - 0.203)  0.031 (0.031 - 0.031)   \n",
       "             Common      0.192 (0.191 - 0.191)  0.031 (0.030 - 0.030)   \n",
       "             Non-common  0.250 (0.252 - 0.252)  0.030 (0.031 - 0.031)   \n",
       "biogpt_all   Total       0.327 (0.317 - 0.337)  0.127 (0.121 - 0.134)   \n",
       "             Common      0.371 (0.362 - 0.381)  0.150 (0.142 - 0.157)   \n",
       "             Non-common  0.242 (0.219 - 0.263)  0.085 (0.073 - 0.095)   \n",
       "baseline_all Total       0.180 (0.175 - 0.175)  0.029 (0.028 - 0.028)   \n",
       "             Common      0.166 (0.167 - 0.167)  0.028 (0.028 - 0.028)   \n",
       "             Non-common  0.239 (0.236 - 0.236)  0.032 (0.031 - 0.031)   \n",
       "\n",
       "                                        METEOR                ROUGE_L  \\\n",
       "Model        Subset                                                     \n",
       "biogpt_he    Total       0.183 (0.179 - 0.186)  0.288 (0.280 - 0.296)   \n",
       "             Common      0.194 (0.190 - 0.198)  0.303 (0.294 - 0.312)   \n",
       "             Non-common  0.154 (0.149 - 0.160)  0.224 (0.214 - 0.235)   \n",
       "baseline_he  Total       0.186 (0.185 - 0.185)  0.163 (0.161 - 0.161)   \n",
       "             Common      0.191 (0.189 - 0.189)  0.166 (0.168 - 0.168)   \n",
       "             Non-common  0.170 (0.171 - 0.171)  0.152 (0.153 - 0.153)   \n",
       "biogpt_all   Total       0.178 (0.174 - 0.183)  0.326 (0.319 - 0.333)   \n",
       "             Common      0.199 (0.194 - 0.203)  0.349 (0.341 - 0.356)   \n",
       "             Non-common  0.140 (0.132 - 0.147)  0.226 (0.214 - 0.240)   \n",
       "baseline_all Total       0.167 (0.164 - 0.164)  0.159 (0.160 - 0.160)   \n",
       "             Common      0.177 (0.178 - 0.178)  0.163 (0.162 - 0.162)   \n",
       "             Non-common  0.144 (0.146 - 0.146)  0.140 (0.139 - 0.139)   \n",
       "\n",
       "                                         CIDEr  \n",
       "Model        Subset                             \n",
       "biogpt_he    Total       0.357 (0.301 - 0.413)  \n",
       "             Common      0.424 (0.354 - 0.493)  \n",
       "             Non-common  0.069 (0.042 - 0.109)  \n",
       "baseline_he  Total       0.012 (0.012 - 0.012)  \n",
       "             Common      0.013 (0.013 - 0.013)  \n",
       "             Non-common  0.008 (0.008 - 0.008)  \n",
       "biogpt_all   Total       0.291 (0.249 - 0.339)  \n",
       "             Common      0.343 (0.289 - 0.404)  \n",
       "             Non-common  0.068 (0.045 - 0.095)  \n",
       "baseline_all Total       0.010 (0.009 - 0.009)  \n",
       "             Common      0.010 (0.010 - 0.010)  \n",
       "             Non-common  0.009 (0.009 - 0.009)  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data list with model names, subsets, result dictionaries, and confidence interval DataFrames\n",
    "data = [\n",
    "    ('biogpt_he', 'Total', he_result, total_he),\n",
    "    ('biogpt_he', 'Common', common_he_result, common_he),\n",
    "    ('biogpt_he', 'Non-common', non_common_he_result, non_common_he),\n",
    "    ('baseline_he', 'Total', he_baseline_result, total_he_baseline),\n",
    "    ('baseline_he', 'Common', common_he_baseline_result, common_he_baseline),\n",
    "    ('baseline_he', 'Non-common', non_common_he_baseline_result, non_common_he_baseline),\n",
    "    ('biogpt_all', 'Total', all_result, total_all),\n",
    "    ('biogpt_all', 'Common', common_all_result, common_all),\n",
    "    ('biogpt_all', 'Non-common', non_common_all_result, non_common_all),\n",
    "    ('baseline_all', 'Total', all_baseline_result, total_all_baseline),\n",
    "    ('baseline_all', 'Common', common_all_baseline_result, common_all_baseline),\n",
    "    ('baseline_all', 'Non-common', non_common_all_baseline_result, non_common_all_baseline),\n",
    "]\n",
    "\n",
    "# Define the metrics to include in the table\n",
    "metrics = ['BLEU_1', 'BLEU_4', 'METEOR', 'ROUGE_L', 'CIDEr']\n",
    "\n",
    "# Create a list to hold the table rows\n",
    "rows = []\n",
    "\n",
    "# Populate the rows with formatted metric values and confidence intervals\n",
    "for model, subset, result_dict, ci_df in data:\n",
    "    row = {'Model': model, 'Subset': subset}\n",
    "    for metric in metrics:\n",
    "        value = result_dict[metric]\n",
    "        lower = ci_df.loc['lower', metric]\n",
    "        upper = ci_df.loc['upper', metric]\n",
    "        # Format the string as \"value (lower - upper)\" with 3 decimal places\n",
    "        formatted = f\"{value:.3f} ({lower:.3f} - {upper:.3f})\"\n",
    "        row[metric] = formatted\n",
    "    rows.append(row)\n",
    "\n",
    "# Create a DataFrame from the rows\n",
    "df_table = pd.DataFrame(rows)\n",
    "\n",
    "# Set the multi-index using 'Model' and 'Subset'\n",
    "df_table.set_index(['Model', 'Subset'], inplace=True)\n",
    "\n",
    "# Display the table\n",
    "df_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
